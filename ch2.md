## 模型评估与选择
### 经验误差与过拟合
- 错误率：如果在$m$个样本中有$a$个样本分类错误，则错误率$E=a/m$
- 精度：$精度=1-错误率$
- 误差：学习器的实际预测输出与样本的真实输出之间的差异
- 训练误差（经验误差）：学习器在训练集上的误差
- 泛化误差：在新样本上的误差
- 过拟合：把训练样本自身的一些特点当作所有潜在样本都具有的一般性质，导致泛化性能下降
- 欠拟合：对训练样本的一般性质尚未学好

### 评估方法
通过实验测试来对学习器的泛华误差进行评估并进而做出选择。
数据集$D={(x_1,y_1),(x_2,y_2),\ldots,(x_m,y_m)}$，处理得到训练集$S$和测试集$T$。

#### 留出法
- 将数据集$D$划分为两个互斥的集合，即$D=S\cup T,S\cap T=\emptyset$.
- 训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差对最终结果产生影响。
- 将大约$2/3$~$4/5$的样本用于训练，剩余样本用于测试。

- 分层采样：保留类别比例的采样方式。

#### 交叉验证法（k折交叉验证）
1. 将数据集$D$划分为$k$个大小相似的互斥子集，即$D=D_1\cup D_2\cup \ldots\cup D_k$，$D_i \cap D_j = \emptyset (i \neq j)$
2. 每次用$k-1$个子集的并集作为训练集，余下的子集作为测试集，即得到$k$组训练/测试集，返回$k$个测试结果的均值。

- 每个子集$D_i$都尽可能保持数据分布的一致性，即从$D$中通过分层采样得到。
- 为减小因样本划分不同而引入的差别，$k$折交叉验证通常要随机使用不同的划分重复$p$次，最终的评估结果是$p$次$k$折交叉验证结果的均值。

- 特例（留一法）：假定数据集$D$中包含$m$个样本，若令$k=m$

#### 自助法（包外估计）
给定包含$m$个样本的数据集$D$，采样产生数据集$D'$：每次随机从$D$中挑选一个样本，将其拷贝放入$D'$，然后再将该样本放回到初始数据集$D$中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行$m$次，得到包含$m$个样本的数据集$D'$。样本在$m$次采样中始终不被采到的概率是$(1-\frac{1}{m})^m$，取极限得到
$${\lim_{m \to \infty}(1-\frac{1}{m})^m \to \frac{1}{e} \approx 0.368}$$
可将$D'$作为训练集，${D}\backslash{D'}$作为测试集。
- 实际评估的模型与期望评估的模型都是用$m$个测试样本，而仍有数据总量约$1/3$的没有在训练集中出现的样本用于测试。
- 自助法在数据集较小、难以有效划分训练集/测试集时很有用。
- 自助法产生的数据改变了初始数据集的分布，会引入估计偏差。

#### 调参与最终模型
- 验证集：模型评估与选择中用于评估测试的数据集。
- 研究对比不同算法的泛化性能时，用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。

### 性能度量
性能度量：衡量模型泛化能力。

在预测任务中，给定样例集$D={(x_1,y_1),(x_2,y_2),\ldots,(x_m,y_m)}$，其中$y_i$是示例$x_i$的真实标记。要评估学习器$f$的性能，就要把学习器预测结果$f(x)$与真实标记$y$进行比较。

回归任务最常用的性能度量是均方误差
$$E(f;D)=\frac{1}{m}\sum^m_{i=1}(f(x_i)-y_i)^2$$

对于数据分布$\mathcal D$和概率密度函数$p(·)$均方误差可描述为
$$E(f;\mathcal D)=\int_{x\backsim\mathcal D}(f(x)-y)^2p(x)\mathrm{d}x$$

#### 错误率与精度
- 错误率
$$E(f;\mathcal D)=\int_{x\backsim\mathcal D}\mathrm{II}(f(x)\neq y)p(x)\mathrm{d}x$$
- 精度
$$\operatorname{acc}(f;\mathcal D)=\int_{x\backsim\mathcal D}\mathrm{II}(f(x)=y)p(x)\mathrm{d}x\
=1-E(f;\mathcal D)$$

#### 查准率、查全率与F1
$$样例总数=TP（真正例）+FP（假正例）+TN（真反例）+FN（假反例）$$

- 查准率P
$$P=\frac{TP}{TP+FP}$$

- 查全率R
$$R=\frac{TP}{TP+FN}$$

- 查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。

#### ROC与AUC
- ROC曲线
    - 真正例率TPR（纵轴）
$$TPR=\frac{TP}{TP+FN}$$
    - 假正例率FPR（横轴）
$$FPR=\frac{FP}{TN+FP}$$

- AUC:ROC曲线下的面积

####  代价敏感错误率与代价曲线
- 代价敏感错误率
$$E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in D^+}\mathrm{II}(f(x_i)\neq y_i)\times {cost}_{01}+\
\sum_{x_i \in D^-}\mathrm{II}(f(x_i)\neq y_i)\times {cost}_{10})$$
- 代价曲线
    - 横轴：取值为$[0,1]$的正例概率代价
$$P(+){cost}=\frac{p \times {cost}_{01}}{p \times {cost}_{01} + (1-p) \times {cost}_{10}}$$
    - 纵轴：取值为$[0,1]$的归一化代价
$${cost}_{norm}=\frac{\mathrm{FNR} \times p \times {cost}_{01} + \mathrm{FNR} \times (1-p) \times {cost}_{10}}{p \times {cost}_{01} + (1-p) \times {cost}_{10}}$$

### 比较检验
- 错误率：$\epsilon$

- 泛化错误率为$\epsilon$的学习器被测得测试错误率为$\hat{\epsilon}$的概率
$$P(\hat{\epsilon};\epsilon)=\dbinom{m}{\hat{\epsilon}\times m}\epsilon^{\hat{\epsilon}\times m}\
(1-\epsilon)^{m-\hat{\epsilon}\times m}$$

- 交叉验证t检验：若两个学习器的性能相同，则他们使用相同的训练集/测试集得到的测试错误率应相同。

### 偏差与方差
#### 回归任务
- 学习算法的期望预测
$$\bar{f}(x)=\mathbb{E}_D[f(x;D)]$$
- 使用样本数相同的不同训练集产生的方差为
$$var(x)=\mathbb{E}_D[(f(x;D)-\bar{f}(x))^2]$$
- 噪声为
$${\varepsilon}^2=\mathbb{E}_D[(y_D-y)^2]$$
- 期望输出与真实标记的差别称为偏差
$${bias}^2(x)=(\bar{f}(x)-y)^2$$
- 期望泛化误差课分解为偏差、方差与噪声之和
$$E(f;D)={bias}^2(x)+{var}(x)+{\varepsilon}^2$$

    - 偏差度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的拟合能力
    - 方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响
    - 噪声表达了当前任务上任何学习算法所能达到的期望泛化误差的下届，刻画了学习问题本身的难度
